{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "In_class_exercise_08 .ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/divyaseri/divya_INFO5731_Fall2020/blob/master/In_class_exercise_08_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KMCMaer19Iu"
      },
      "source": [
        "# **The eighth in-class-exercise (20 points in total, 10/29/2020)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85-wmb_J19Iz"
      },
      "source": [
        "The data for this exercise is from the dataset you created from assignment three. Please perform answer the following questions based on your data:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIj3Pskf19I2"
      },
      "source": [
        "## (1) (10 points) Write a python program to extract the sentiment related terms from the corpus. You may use python package such as polyglot or external lexicon resources in the question. Rank the sentiment related terms by frequency."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzX5wvZG19I5",
        "outputId": "bb1ef87c-df30-4e25-d60c-90e2fe83f2c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install polyglot "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting polyglot\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/98/e24e2489114c5112b083714277204d92d372f5bbe00d5507acf40370edb9/polyglot-16.7.4.tar.gz (126kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 18.8MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 20kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 30kB 3.6MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 40kB 4.1MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 51kB 2.5MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 61kB 3.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 71kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 81kB 3.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 92kB 4.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 102kB 3.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 112kB 3.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 122kB 3.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 3.6MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: polyglot\n",
            "  Building wheel for polyglot (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for polyglot: filename=polyglot-16.7.4-py2.py3-none-any.whl size=52559 sha256=195601c8549c6936701f3005f9e8e344cc9630120cdc4b1172f848b5e88d79c5\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/91/ef/f1369fdc1203b0a9347d4b24f149b83a305f39ab047986d9da\n",
            "Successfully built polyglot\n",
            "Installing collected packages: polyglot\n",
            "Successfully installed polyglot-16.7.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTq7qpXNqeS7",
        "outputId": "c113e61f-9c15-412f-97ba-740c90fe96b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install Morfessor"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting Morfessor\n",
            "  Downloading https://files.pythonhosted.org/packages/39/e6/7afea30be2ee4d29ce9de0fa53acbb033163615f849515c0b1956ad074ee/Morfessor-2.0.6-py3-none-any.whl\n",
            "Installing collected packages: Morfessor\n",
            "Successfully installed Morfessor-2.0.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uA1zF9J4qe9_",
        "outputId": "ae83551b-437c-4751-b00c-5a486ce47ca7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install pyicu"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyicu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5a/99/c48c816095208bf3f4936ff67e571621fbddef461303a35a076f234e31f6/PyICU-2.5.tar.gz (225kB)\n",
            "\r\u001b[K     |█▌                              | 10kB 16.1MB/s eta 0:00:01\r\u001b[K     |███                             | 20kB 2.9MB/s eta 0:00:01\r\u001b[K     |████▍                           | 30kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 40kB 3.9MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 51kB 3.2MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 61kB 3.7MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 71kB 3.9MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 81kB 4.2MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 92kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 102kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████                | 112kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 122kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 133kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 143kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 153kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 163kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 174kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 184kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 194kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 204kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 215kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 225kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 235kB 4.4MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyicu\n",
            "  Building wheel for pyicu (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyicu: filename=PyICU-2.5-cp36-cp36m-linux_x86_64.whl size=1252546 sha256=6cdae00a60e5e9ce55e9f27d5da6eeda94640a6a83f8f47c95b5b824d3ed15c3\n",
            "  Stored in directory: /root/.cache/pip/wheels/07/3a/28/09f90c38785945ddf9af61b7add1aa62a740f40e259626ef3a\n",
            "Successfully built pyicu\n",
            "Installing collected packages: pyicu\n",
            "Successfully installed pyicu-2.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1SB8IbIqfMq",
        "outputId": "ac78dc83-9a32-4f42-cb93-41f25321ce45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install pycld2"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pycld2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/21/d2/8b0def84a53c88d0eb27c67b05269fbd16ad68df8c78849e7b5d65e6aec3/pycld2-0.41.tar.gz (41.4MB)\n",
            "\u001b[K     |████████████████████████████████| 41.4MB 99kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pycld2\n",
            "  Building wheel for pycld2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycld2: filename=pycld2-0.41-cp36-cp36m-linux_x86_64.whl size=9833491 sha256=c3ea4e96949e6d01404248137edeca4e2f8ec59a41432e4b7a33a602e997684d\n",
            "  Stored in directory: /root/.cache/pip/wheels/c6/8f/e9/08a1a8932a490175bd140206cd86a3dbcfc70498100de11079\n",
            "Successfully built pycld2\n",
            "Installing collected packages: pycld2\n",
            "Successfully installed pycld2-0.41\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPXRa68HrD5R",
        "outputId": "759b7012-9e8e-46eb-f876-e4b314958e3c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from polyglot.text import Text\n",
        "df=pd.read_csv(\"/content/Review_file.csv\")\n",
        "data=[]\n",
        "# 1.To remove special characters, punctuations and numbers\n",
        "df[\"Review\"] = df[\"Review\"].str.replace(r\"\\W\", \" \").str.strip()\n",
        "df[\"Review\"]= df[\"Review\"].str.replace(r'\\d+',\"\")\n",
        "for a in df[\"Review\"]:   \n",
        "   data.append(a)\n",
        "df['data']=pd.DataFrame(data)\n",
        "blob = df['data'].to_string()\n",
        "text = Text(blob)\n",
        "tags=text.pos_tags\n",
        "#tags\n",
        "sent_word = [x for (x,y) in tags if y not in ('NUM','PUNCT','ADP','AUX','CONJ','DET',\n",
        "                                              'INTJ','NOUN','PART','PRON','PROPN',\n",
        "                                              'SCONJ','SYM','X','VERB','ADV')]\n",
        "sent_word"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Best',\n",
              " 'de',\n",
              " 'early',\n",
              " 'sad',\n",
              " 'good',\n",
              " 'recurring',\n",
              " 'many',\n",
              " 'intense',\n",
              " 'more',\n",
              " 'perfect',\n",
              " 'Amazing',\n",
              " 'magnificent',\n",
              " 'complete',\n",
              " 'worthy',\n",
              " 'Joaquin',\n",
              " 'best',\n",
              " 'few',\n",
              " 'unbelievable',\n",
              " 'high',\n",
              " 'incredible',\n",
              " 'unconvincing',\n",
              " 'best',\n",
              " 'mental',\n",
              " 'short',\n",
              " 'cinematic',\n",
              " 'good',\n",
              " 'sure',\n",
              " 'Creative',\n",
              " 'compelling',\n",
              " 'new',\n",
              " 'Absolute',\n",
              " 'silly',\n",
              " 'Best',\n",
              " 'first',\n",
              " 'Amazing',\n",
              " 'enjoyable',\n",
              " 'Dark',\n",
              " 'entertaining',\n",
              " 'dark',\n",
              " 'perfect',\n",
              " 'best',\n",
              " 'dark',\n",
              " 'understandable',\n",
              " 'interesting',\n",
              " 'real',\n",
              " 'brilliant',\n",
              " 'hyped',\n",
              " 'unique',\n",
              " 'more',\n",
              " 'outstanding',\n",
              " 'strong',\n",
              " 'stunning',\n",
              " 'lite',\n",
              " 'excellent',\n",
              " 'amazing',\n",
              " 'first',\n",
              " 'unconvincing',\n",
              " 'excellent',\n",
              " 'good']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zgt9hKLwsJN9",
        "outputId": "1297395f-f6dd-4ffb-cb79-2c9d336879b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import collections\n",
        "from operator import itemgetter\n",
        "N = 200\n",
        "frequency = collections.Counter(sent_word)\n",
        "top_words = sorted(frequency.items(), key=itemgetter(1), reverse=True)[:N]\n",
        "for i, (word, frequency) in enumerate(top_words, start=1):\n",
        "    print(\"%d %s %d\" % (i, word, frequency))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 good 3\n",
            "2 best 3\n",
            "3 Best 2\n",
            "4 more 2\n",
            "5 perfect 2\n",
            "6 Amazing 2\n",
            "7 unconvincing 2\n",
            "8 first 2\n",
            "9 dark 2\n",
            "10 excellent 2\n",
            "11 de 1\n",
            "12 early 1\n",
            "13 sad 1\n",
            "14 recurring 1\n",
            "15 many 1\n",
            "16 intense 1\n",
            "17 magnificent 1\n",
            "18 complete 1\n",
            "19 worthy 1\n",
            "20 Joaquin 1\n",
            "21 few 1\n",
            "22 unbelievable 1\n",
            "23 high 1\n",
            "24 incredible 1\n",
            "25 mental 1\n",
            "26 short 1\n",
            "27 cinematic 1\n",
            "28 sure 1\n",
            "29 Creative 1\n",
            "30 compelling 1\n",
            "31 new 1\n",
            "32 Absolute 1\n",
            "33 silly 1\n",
            "34 enjoyable 1\n",
            "35 Dark 1\n",
            "36 entertaining 1\n",
            "37 understandable 1\n",
            "38 interesting 1\n",
            "39 real 1\n",
            "40 brilliant 1\n",
            "41 hyped 1\n",
            "42 unique 1\n",
            "43 outstanding 1\n",
            "44 strong 1\n",
            "45 stunning 1\n",
            "46 lite 1\n",
            "47 amazing 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgDkrOSg19JK"
      },
      "source": [
        "## (2) (10 points) Compare the performance of the following tools in sentiment identification: TextBlob (https://textblob.readthedocs.io/en/dev/), VADER (https://github.com/cjhutto/vaderSentiment), TFIDF-based Support Vector Machine (SVM) (Split your data into training and testing data). Take your own annotation as the standard answers. \n",
        "\n",
        "Reference code: https://towardsdatascience.com/fine-grained-sentiment-analysis-in-python-part-1-2697bb111ed4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRxVsA4y19JN",
        "outputId": "a8240a75-17db-409d-dde4-1e47dc9a658d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import nltk\n",
        "#Textblob:\n",
        "nltk.download('vader_lexicon')\n",
        "import textblob\n",
        "from textblob import TextBlob\n",
        "testimonial = TextBlob((df['Review'])[1])\n",
        "print(testimonial.sentiment)\n",
        "#vader:\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "vader=SentimentIntensityAnalyzer()\n",
        "print(vader.polarity_scores((df['Review'])[1]))\n",
        "print(\"actual sentiment:\",(df['Sentiment'])[1])\n",
        "print(\"actual sentence:\",(df['Review'])[1])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "Sentiment(polarity=-0.03125, subjectivity=0.71875)\n",
            "{'neg': 0.12, 'neu': 0.649, 'pos': 0.232, 'compound': 0.9042}\n",
            "actual sentiment: Positive\n",
            "actual sentence: This is a movie that only those who have felt alone and isolated can truly relate to it  You understand the motive and you feel sorry for the character  A lot of people will see this movie and think that it encourages violence  But truly  this movie should encourage each and every one of us to become a better person  treat everyone with respect and make each other feel like they belong in this world  instead of making them feel isolated\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}